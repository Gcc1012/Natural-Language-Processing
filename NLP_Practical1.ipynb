{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq/CeL+DrDYXA9iVdjWSmv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gcc1012/Natural-Language-Processing/blob/main/NLP_Practical1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AIM: To perform conceptual and practical understanding of basic NLP libraries"
      ],
      "metadata": {
        "id": "PAktGgucMjf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CountVectorizer: CountVectorizer means breaking down a sentence or any text into words by performing pre-processing tasks like converting all words to lowercase, thus removing special characters.The default tokenization in Count Vectorizer removes all special characters, punctuation and single characters."
      ],
      "metadata": {
        "id": "56tT0ZZA_mKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anEbdBPk2TH2"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "stmt = [\"The weather is nice. The weather is cold.\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec.fit_transform(stmt).todense()\n",
        "##first it alphabetically organizes the word and then removes all punctuations,special characters,single characters "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZdkgYLf8uxV",
        "outputId": "e4dfc6ed-21ba-443a-f2df-4762733514bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1, 2, 1, 2, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Including Stopwords"
      ],
      "metadata": {
        "id": "CXqo5Vl0DhHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "stmt  = [\"The weather is nice. The weather is cold.\"]\n",
        "vec = CountVectorizer(stop_words='english')\n",
        "vec.fit_transform(stmt).todense()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVGi7CRc8uYZ",
        "outputId": "f2653cce-ba70-4cf3-c4f2-10c2ee309ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1, 1, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NLTK -Natural Language Toolkit\n",
        "NLTK (Natural Language Toolkit) is the go-to API for NLP (Natural Language Processing) with Python. It is a really powerful tool to preprocess text data for further analysis like with ML models for instance.\n",
        "\n"
      ],
      "metadata": {
        "id": "g4d4I8H9__oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkZjg2-7_Rz1",
        "outputId": "2924e0fc-4dc2-479c-d800-28dd8ae519f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stmt  = \"Weather is nice,Weather is cold.\""
      ],
      "metadata": {
        "id": "tK-Hu5GnAMpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenize the sentence"
      ],
      "metadata": {
        "id": "b9VxBJfRCxqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(stmt)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgqQMoRsAYEl",
        "outputId": "93a9074e-d6b6-4f56-9ab0-c82b29f5aa89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Weather', 'is', 'nice', ',', 'Weather', 'is', 'cold', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stopwords"
      ],
      "metadata": {
        "id": "OqxKw9D3CkxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ufi-B50CBdT",
        "outputId": "befa29e5-e893-4fb6-d118-fad82d5abcc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = nltk.corpus.stopwords.words('english')\n",
        "print(sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEFpkDubCLLw",
        "outputId": "c625b300-372f-4083-f72b-1cb654945d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozFAb0YjCz9O",
        "outputId": "028380c5-fb64-4b8c-b008-3e8fa90be79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stmt1 = \"I have a pet dog.\"\n",
        "tokens = word_tokenize(stmt1)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYqbgjXTDVyj",
        "outputId": "93a59368-a29c-461f-bf7e-fe0b4bb0fcd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'have', 'a', 'pet', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1  = tokens"
      ],
      "metadata": {
        "id": "lVdCU5wdEVIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l2 = []\n",
        "for i in l1:\n",
        "  i=i.lower()\n",
        "  if i not in sw:\n",
        "     l2.append(i)\n",
        "l2     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS178x3ED9DM",
        "outputId": "f6a64685-056f-499a-805f-e10884360dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pet', 'dog', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2 = [i for i in l1 if i not in sw ]\n",
        "\n",
        "l2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reubDq7AFyco",
        "outputId": "2c707279-bb94-4f51-c6d1-4f2330cf0a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'pet', 'dog', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wordnet\n",
        "The WordNet is a part of Python's Natural Language Toolkit. It is a large word database of English Nouns, Adjectives, Adverbs and Verbs. These are grouped into some set of cognitive synonyms, which are called synsets"
      ],
      "metadata": {
        "id": "M-8a_qKDHfT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtEamcRaHiZg",
        "outputId": "309a94cb-adc5-46a4-80b3-3306b9139058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC28Jo6oKk8s",
        "outputId": "e7c0c982-d198-4984-a002-2be33c6ff9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lemmatization"
      ],
      "metadata": {
        "id": "gcl3YHFtBWrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WordNetLemmatizer\n",
        "WordNetLemmatizer is a class in the Natural Language \n",
        "Toolkit (NLTK) library in Python that is used to perform lemmatization on \n",
        "words. Lemmatization is a process of reducing a word to its base or root form, \n",
        "known as the lemma.\n",
        "For example, the lemma of the word \"running\" is \"run\". In addition to this, it gives \n",
        "more fine-grained and precise output than stemming."
      ],
      "metadata": {
        "id": "wwPeW_fmAjpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ZaomC3fJHm4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stmt2 = 'humans cats dogs rats caring'\n",
        "token1 = word_tokenize(stmt2)\n",
        "print(token1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZJS66-hJ3Lx",
        "outputId": "3c01ca1d-73e4-4064-d12a-e5d1cb6de2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['humans', 'cats', 'dogs', 'rats', 'caring']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l3 = [wnl.lemmatize(i) for i in token1]\n",
        "print(l3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfQ-krdbKLVb",
        "outputId": "4e0ed707-50fa-463f-ae4e-49293b29f2b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['human', 'cat', 'dog', 'rat', 'caring']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stmt2 = 'caring'\n",
        "token1 = word_tokenize(stmt2)\n",
        "print(token1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2cWuvKQpnx3",
        "outputId": "81909413-5074-429e-bf47-5ddfafeb3e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['caring']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l3 = [wnl.lemmatize(i) for i in token1]\n",
        "print(l3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8puwaUvpuH6",
        "outputId": "59aca99a-926a-4968-d71d-f019d52054df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['caring']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stmt = \"bunch of books !\"\n",
        "token = word_tokenize(stmt)\n",
        "print(token)\n",
        "l =[wnl.lemmatize(i) for i in token]\n",
        "l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yFatLsGwKkg",
        "outputId": "26817216-a8e4-4519-e16c-840677b7cb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bunch', 'of', 'books', '!']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bunch', 'of', 'book', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "text = \"The running dogs chased the escaping cats. \"\n",
        "token = word_tokenize(text)\n",
        "print(token)\n",
        "l1 =[lemmatizer.lemmatize(i) for i in token]\n",
        "l1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXWHSUPftiB1",
        "outputId": "1c12d433-5b81-4b57-a207-b2aa5082a141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'running', 'dogs', 'chased', 'the', 'escaping', 'cats', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'running', 'dog', 'chased', 'the', 'escaping', 'cat', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming"
      ],
      "metadata": {
        "id": "U5Cia8gRBcYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PorterStemmer\n",
        " The Porter stemmer is an algorithm for reducing words to their \n",
        "root form, known as the stem.\n",
        "For example, the stem of the word \"jumping\" is \"jump\", and the stem of the word \n",
        "\"jumps\" is also \"jump\". The stemmer is able to identify the common root \"jump\" by \n",
        "removing the \"ing\" and \"s\" suffixes from the words."
      ],
      "metadata": {
        "id": "X138cq8BBeEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "wKddgTo4MI72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stmt4 = \"She is so caring\""
      ],
      "metadata": {
        "id": "ZmhprAUXMXHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token2 = word_tokenize(stmt4)\n",
        "print(token2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sVyueVWM9Hx",
        "outputId": "e3b9e3d8-5efd-4b80-d287-f7d80ab6bfb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['She', 'is', 'so', 'caring']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l4=[ps.stem(i) for i in token2]\n",
        "l4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmIOhTOsOAoC",
        "outputId": "ce5598a0-f243-44c6-9f02-109ab601aa6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['she', 'is', 'so', 'care']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "txt = \"jumping jumps jump\"\n",
        "token = word_tokenize(txt)\n",
        "print(token)\n",
        "l1=[ps.stem(i) for i in token]\n",
        "l1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxGKU7ys2wJL",
        "outputId": "e5d85cbb-bf7c-41ac-8b93-b03b5f7793c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jumping', 'jumps', 'jump']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jump', 'jump', 'jump']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps =PorterStemmer()\n",
        "txt = \"flattering funny showing involvement shoes hands\"\n",
        "token = word_tokenize(txt)\n",
        "print(token)\n",
        "l1 = [ps.stem(i) for i in token]\n",
        "l1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIBGhlWi5QoZ",
        "outputId": "ac4060e3-95c5-4a07-df51-c4fadf2fc093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['flattering', 'funny', 'showing', 'involvement', 'shoes', 'hands']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['flatter', 'funni', 'show', 'involv', 'shoe', 'hand']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###averaged_perceptron_tagger is used for tagging words with their parts of speech (POS)"
      ],
      "metadata": {
        "id": "S_R9L9sODLdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBqCLOcoORUz",
        "outputId": "65b2dc2f-e39e-4e99-8aa2-6ffa88437a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part-of-speech (POS) tagging \n",
        "It is a process of converting a sentence to forms –\n",
        "list of words, list of tuples (where each tuple is having a form (word, tag)). The tag \n",
        "in case of is a part-of-speech tag, and signifies whether the word is a noun, \n",
        "adjective, verb, and so on.\n"
      ],
      "metadata": {
        "id": "nTYJdf5EBmGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag([\"beautiful\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOEeZvngOIyh",
        "outputId": "66a59515-e248-48b1-b5ab-6f457585eed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('beautiful', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis using TextBlob"
      ],
      "metadata": {
        "id": "auc9XD-R9t4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####TextBlob: TextBlob is a simple library which supports complex analysis and operations on textual data. TextBlob returns polarity and subjectivity of a sentence. \n",
        "####Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment.\n",
        "For example — emoticons, exclamation mark, emojis, etc.\n",
        "####Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information.\n",
        "For example: We calculated polarity and subjectivity for “I do not like this example at all, it is too boring”. For this particular example, polarity = -1 and subjectivity is 1, \n",
        "which is fair"
      ],
      "metadata": {
        "id": "SqaT5JPR_D6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "ZIisXj1i9vhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = TextBlob(\"The weather is amazing.\")\n",
        "print(txt.sentiment.polarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5J_xmig-kGS",
        "outputId": "a076b28a-f075-4359-c41f-700761e4051a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6000000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(\"The weather is amazing.\").sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3T8mOSL_IjI",
        "outputId": "f63c69f0-3c22-4569-c5a4-e0d840efb95e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.6000000000000001, subjectivity=0.9)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RKgwafkyATMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(\"The weather is amazing excellent outstanding awesome.\").sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YasHXV47_Rpa",
        "outputId": "7a8befb0-3e74-4ada-ef3c-975ca748f09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.775, subjectivity=0.94375)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(\"Today's weather is awesome.\").sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLz8S_ma-CZf",
        "outputId": "9a8c97c4-177b-4361-f098-c7a2920e55a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=1.0, subjectivity=1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(\"I love maggie\").sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bARx7Ziv-Pvz",
        "outputId": "266569cf-ea86-45b5-bb6f-83e73dc324a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.5, subjectivity=0.6)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(\"This is dangerous\").sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcM3kkGt_8Ll",
        "outputId": "f0d44e08-bf85-4e3e-f6aa-c769ab9981ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=-0.6, subjectivity=0.9)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}