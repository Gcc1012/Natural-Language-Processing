{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0HNTKmUEOKkVI/c650d/i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gcc1012/Natural-Language-Processing/blob/main/NLP_Practical2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset\n",
        "Zomata Reviews dataset has been used on which basic preprocessing techniques are applied.\n",
        "https://drive.google.com/file/d/1NTm1L0tibUpWHyibBUDZKQJCd0M6ZEsm/view?usp=sharing"
      ],
      "metadata": {
        "id": "tvS1Tsz0XXLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AIM: Understand and perform basic text preprocessing techniques and extracting meaningful information from it."
      ],
      "metadata": {
        "id": "U7BV0oD6A5_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization"
      ],
      "metadata": {
        "id": "Pol7vAYDBVmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnLTDm6bAyYM",
        "outputId": "c25c4bbd-143d-4b63-cb6f-126f7c00ee43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Today', 'is', 'Wednesday.']\n"
          ]
        }
      ],
      "source": [
        "txt = \"Today is Wednesday.\".split()\n",
        "print(txt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Generally the cost of rolex watches is in $100 to $300\".split()\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFCfNCxIBp-4",
        "outputId": "fbeae10d-6adb-41f4-87e3-7424f9a989f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Generally', 'the', 'cost', 'of', 'rolex', 'watches', 'is', 'in', '$100', 'to', '$300']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt =  \"This is my #OOTD\".split()\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c5P2E7QCq7_",
        "outputId": "9bb60e52-d49b-4b1b-df09-13fff39d33d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'my', '#OOTD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Once you stop Learning you start Dying!!!!!!\".split()\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V515nP09DSQ5",
        "outputId": "ed301e25-bee3-4df2-8313-20d28cf7592b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Once', 'you', 'stop', 'Learning', 'you', 'start', 'Dying!!!!!!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"PLease wake up @Khushi\".split()\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QQrFzxlDhf4",
        "outputId": "052a5c36-9740-4416-beb6-1f90fdf340e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PLease', 'wake', 'up', '@Khushi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Venus sir is our Proctor. Venus is planet in solar system. \".split()\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEA5GylEDtcu",
        "outputId": "aaa5988c-a0dd-4c71-9f0b-cadc48c6672c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Venus', 'sir', 'is', 'our', 'Proctor.', 'Venus', 'is', 'planet', 'in', 'solar', 'system.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Regular Expression Tokenizer\n",
        "\n",
        "####A RegexpTokenizer splits a string into substrings using a regular expression."
      ],
      "metadata": {
        "id": "3-k1vz4OEoNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\\w --> word character\n",
        "###\\s -->Matches whitespaces\n",
        "###\\S-->Matches non-whitespaces"
      ],
      "metadata": {
        "id": "Y_RV_LTCFV0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "d0peqVE-EtKI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####example -->the following tokenizer forms tokens out of alphabetic sequences, money expressions, and any other non-whitespace sequences:"
      ],
      "metadata": {
        "id": "o8ovgS3eFp2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
        "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
        "tokenizer.tokenize(txt) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM68JFX4EB_h",
        "outputId": "7f8c6a94-ecef-47e4-83a2-6c65a415cb0f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good',\n",
              " 'muffins',\n",
              " 'cost',\n",
              " '$3.88',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York',\n",
              " '.',\n",
              " 'Please',\n",
              " 'buy',\n",
              " 'me',\n",
              " 'two',\n",
              " 'of',\n",
              " 'them',\n",
              " '.',\n",
              " 'Thanks',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "capital_words = RegexpTokenizer(r'[A-Z]\\w')\n",
        "capital_words.tokenize(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_VEbqO8GTWC",
        "outputId": "6e5df427-7f29-4a59-939d-c4d1a056423f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go', 'Ne', 'Yo', 'Pl', 'Th']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "capital_words = RegexpTokenizer(r'[A-Z]\\w+')\n",
        "capital_words.tokenize(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mou4_Lg-ILBG",
        "outputId": "9ab67f32-892a-4984-8e90-6fd2c24ff3fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good', 'New', 'York', 'Please', 'Thanks']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "s = \"We'll - A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
        "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWbcrTd1HyTK",
        "outputId": "cd048b4f-43ee-454e-953c-b0edb3c51db3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We',\n",
              " \"'ll\",\n",
              " '-',\n",
              " 'A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '$3000.0',\n",
              " '-',\n",
              " '$8000.0',\n",
              " 'in',\n",
              " 'USA',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Blank Line Tokenizer\n",
        "\n",
        "Tokenize a string, treating any sequence of blank lines as a delimiter. Blank lines are defined as lines containing no characters, except for space or tab characters.\n",
        "\n"
      ],
      "metadata": {
        "id": "eJ6kBmcTIaZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import BlanklineTokenizer\n",
        "blt = BlanklineTokenizer()"
      ],
      "metadata": {
        "id": "hrBQnGanI64m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt= \"My name is Karan.\\n I am from Vadodra\""
      ],
      "metadata": {
        "id": "0N3boTVzI1gk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blt.tokenize(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36FDQm9bJFNk",
        "outputId": "6b19f073-149e-42e2-f58e-a933900da1d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My name is Karan.\\n I am from Vadodra']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"I am fine. \\n\\n\\n This is nice.\"\n",
        "blt.tokenize(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qoPL2uvKetO",
        "outputId": "014488b9-a44f-4bb4-98e7-2be62d1ae2a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am fine.', 'This is nice.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word Punctuation Tokenizer\n",
        "With the help of WordPunctTokenizer, we are able to extract the tokens from string of words or sentences in the form of Alphabetic and Non-Alphabetic character, in simple words the special characters,numbers are also tokenized separately."
      ],
      "metadata": {
        "id": "0UqZe6GEKvs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "metadata": {
        "id": "b0IqgZ4WLVUa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk = WordPunctTokenizer()"
      ],
      "metadata": {
        "id": "TgycQAgALZcg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"I have a BMW of cost $1000.50\"\n",
        "x = tk.tokenize(txt)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLO_YrN7Kshb",
        "outputId": "dfeb9ea3-684e-4caf-83de-f862ad6500eb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'have', 'a', 'BMW', 'of', 'cost', '$', '1000', '.', '50']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Treebank Word Tokenizer\n",
        "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. This tokenizer performs the following steps: split standard contractions, e.g. don't -> do n't and they'll -> they 'll."
      ],
      "metadata": {
        "id": "eio_zMykMYJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "zZiY2FbULqz9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbt = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "VNAOdJhYM9gP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"They'll save and invest more.\""
      ],
      "metadata": {
        "id": "wpVfh16zMtuU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = tbt.tokenize(txt)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QC3s8R4NEMX",
        "outputId": "562b941b-7f2d-4b8a-f5ad-a4c8b656b452"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['They', \"'ll\", 'save', 'and', 'invest', 'more', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"I don't want to go to USA\""
      ],
      "metadata": {
        "id": "kMCn6YSzNeUm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y =tbt.tokenize(txt)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvczTIeANzrR",
        "outputId": "02a5dd9b-9c65-47cf-884b-7614e085ca88"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'do', \"n't\", 'want', 'to', 'go', 'to', 'USA']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+|\\S+')\n",
        "tokenizer.tokenize(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6itXTElNjy6",
        "outputId": "36cc377a-eb6f-4883-bb5a-1692ec06899c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'don', \"'t\", 'want', 'to', 'go', 'to', 'USA']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tk.tokenize(txt)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow5LXL89OBYZ",
        "outputId": "ee5013d8-aa7c-4cc3-9cca-11bb6cb25df4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'don', \"'\", 't', 'want', 'to', 'go', 'to', 'USA']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blt.tokenize(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlkfgoFSOJt8",
        "outputId": "4fc071b9-b44e-4d4a-c1e0-a53e964f4f5a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I don't want to go to USA\"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tweet Tokenizer\n",
        "NLTK has this special method called TweetTokenizer() that helps to tokenize Tweet Corpus into relevant tokens. The advantage of using TweetTokenizer() compared to regular word_tokenize is that, when processing tweets, we often come across emojis, hashtags that need to be handled differently.\n"
      ],
      "metadata": {
        "id": "vt4prLmpOaJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "DgjbHGdnON8q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twk = TweetTokenizer(reduce_len = True,strip_handles =True)"
      ],
      "metadata": {
        "id": "sXClTKlROjdG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Hi I am @Gayatri. This is my #ootd.. This is Rolexxxxxxxxxxxxxxxx\"\n",
        "x = twk.tokenize(txt)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjzDa3igOnnk",
        "outputId": "788cec9c-8d63-4ea4-b6f1-f50ea13aa43c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'I', 'am', '.', 'This', 'is', 'my', '#ootd', '..', 'This', 'is', 'Rolexxx']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
        "           'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']\n"
      ],
      "metadata": {
        "id": "p0a4DNhyBTuY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "tt = [ps.stem(i) for i in plurals ]\n",
        "print(tt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-lKKLsoBTlZ",
        "outputId": "2f5f2370-c67c-4b0f-96ba-96314187a0b2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['caress', 'fli', 'die', 'mule', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'tradit', 'refer', 'colon', 'plot', 'have', 'gener']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" \".join(tt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1we-55lCvIu",
        "outputId": "ad6f78e2-aece-4a2b-dac5-17286f80a38f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tt1 = ' '.join(tt)\n",
        "print(tt1)"
      ],
      "metadata": {
        "id": "TY_etaArJAUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78088a19-546a-4ab9-91e3-2d8cccb34cd9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Snowball Stemmer"
      ],
      "metadata": {
        "id": "mQgfYq93DSrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "print(SnowballStemmer.languages)\n"
      ],
      "metadata": {
        "id": "yiLUZVkeDVCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d052e6-093d-4992-db7e-0a6e045324f0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language = 'english'"
      ],
      "metadata": {
        "id": "DAUWYMDLEJ8W"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sb = SnowballStemmer(language = 'english')\n",
        "tt1 = [sb.stem(i) for i in plurals ]\n",
        "print(tt1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZFAbyPIEOL_",
        "outputId": "32cb1a73-5724-43bf-b262-47a946aba055"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['caress', 'fli', 'die', 'mule', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'tradit', 'refer', 'colon', 'plot', 'have', 'generous']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WordNetLemmatizer"
      ],
      "metadata": {
        "id": "fa9T3JoFEeJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt_9illLFCKF",
        "outputId": "55b6d99b-e304-4202-c9af-aa1fb95a2176"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "aDkmi6hREiTP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_7MdF85FHlV",
        "outputId": "4c20eaa3-6ec2-4f82-af1a-58c359df380d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tt2 = [wnl.lemmatize(i) for i in plurals]\n",
        "print(tt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvxNlX1iE1Pd",
        "outputId": "269a1ace-f10b-49f5-d2fb-09ffae1e97bf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['caress', 'fly', 'dy', 'mule', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating', 'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56-2hj2qGxkm",
        "outputId": "6ca1e983-7590-4e2c-8de0-027800b50984"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in plurals :\n",
        "  print(nltk.pos_tag([i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arwhFyHmGeeo",
        "outputId": "b4d5eb7f-d608-4152-9aba-0278faf52176"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('caresses', 'NNS')]\n",
            "[('flies', 'NNS')]\n",
            "[('dies', 'NNS')]\n",
            "[('mules', 'NNS')]\n",
            "[('died', 'VBD')]\n",
            "[('agreed', 'VBD')]\n",
            "[('owned', 'VBN')]\n",
            "[('humbled', 'VBN')]\n",
            "[('sized', 'VBN')]\n",
            "[('meeting', 'NN')]\n",
            "[('stating', 'VBG')]\n",
            "[('siezing', 'VBG')]\n",
            "[('itemization', 'NN')]\n",
            "[('traditional', 'JJ')]\n",
            "[('reference', 'NN')]\n",
            "[('colonizer', 'NN')]\n",
            "[('plotted', 'VBN')]\n",
            "[('having', 'VBG')]\n",
            "[('generously', 'RB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IwUhm2vG1f2",
        "outputId": "07224cbd-ec66-4dec-9a3f-6081aaf8c304"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "NojvyLOzHq5n"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(stopwords.words('english'))\n",
        "print(sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um7n4wNqH5IN",
        "outputId": "7c6f005a-6992-4838-dd8a-6e49ca8a7dba"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ain', 'yourselves', 'herself', 'which', 'through', 'very', 'just', 'few', 'who', 'or', 'from', 'shouldn', 'further', 'ourselves', 'some', 'over', 'up', 'here', 'can', 'have', 'm', \"mustn't\", 'has', 'about', \"you've\", 'there', 'our', \"needn't\", 'does', 'how', 'under', 'was', 'most', 'a', 'is', 'haven', 'wasn', 'as', 'such', 'she', 'no', 'needn', 'where', 'weren', 'then', 'were', 'having', \"wouldn't\", 'her', 'same', 'my', 'doing', 'only', 'yours', 'at', 'against', 'yourself', 'by', 've', 's', 'theirs', 'that', 'itself', 'and', 'to', 'o', 'hasn', 'below', \"wasn't\", 'myself', \"you're\", 'we', 'why', 'other', 're', \"aren't\", 'more', 'didn', 'd', 'after', \"couldn't\", \"you'd\", 'while', 'your', \"it's\", 'both', 'mustn', 'had', 'for', 'doesn', \"shan't\", 'too', 'you', 'are', 'down', 'all', 'won', 'hers', 'should', \"shouldn't\", 'again', 'not', 'll', 'above', 'i', 'because', 'out', \"weren't\", 'during', 'in', 'but', 'those', 'it', 'hadn', \"should've\", 'isn', 'do', 'now', 'me', 'mightn', \"isn't\", 'when', 'couldn', 'be', 'what', 'their', 'whom', 'once', 'will', 'y', 'than', \"that'll\", 'nor', \"mightn't\", 'his', 'this', 'if', 'them', 'been', 'these', 'themselves', \"didn't\", 'of', \"haven't\", \"you'll\", 'wouldn', 'an', \"she's\", 'with', 'into', 'aren', 'shan', 'don', 'he', 'they', 'ma', 'any', 'himself', 'the', 't', 'did', 'until', \"hasn't\", \"don't\", 'ours', 'off', 'so', 'between', 'him', \"hadn't\", 'on', 'being', \"won't\", 'am', \"doesn't\", 'own', 'each', 'its', 'before'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom', 'not']"
      ],
      "metadata": {
        "id": "0CEIfM08IJXw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in wh_words if i not in sw ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75iD1aQ-JHuy",
        "outputId": "f8dccfdb-0a25-45df-ce68-440647b32c9a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in sw if i not in wh_words ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P0lzRCRL-q-",
        "outputId": "7bfb5be0-f976-4dc0-db91-4b472bc296f1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ain',\n",
              " 'yourselves',\n",
              " 'herself',\n",
              " 'through',\n",
              " 'very',\n",
              " 'just',\n",
              " 'few',\n",
              " 'or',\n",
              " 'from',\n",
              " 'shouldn',\n",
              " 'further',\n",
              " 'ourselves',\n",
              " 'some',\n",
              " 'over',\n",
              " 'up',\n",
              " 'here',\n",
              " 'can',\n",
              " 'have',\n",
              " 'm',\n",
              " \"mustn't\",\n",
              " 'has',\n",
              " 'about',\n",
              " \"you've\",\n",
              " 'there',\n",
              " 'our',\n",
              " \"needn't\",\n",
              " 'does',\n",
              " 'under',\n",
              " 'was',\n",
              " 'most',\n",
              " 'a',\n",
              " 'is',\n",
              " 'haven',\n",
              " 'wasn',\n",
              " 'as',\n",
              " 'such',\n",
              " 'she',\n",
              " 'no',\n",
              " 'needn',\n",
              " 'weren',\n",
              " 'then',\n",
              " 'were',\n",
              " 'having',\n",
              " \"wouldn't\",\n",
              " 'her',\n",
              " 'same',\n",
              " 'my',\n",
              " 'doing',\n",
              " 'only',\n",
              " 'yours',\n",
              " 'at',\n",
              " 'against',\n",
              " 'yourself',\n",
              " 'by',\n",
              " 've',\n",
              " 's',\n",
              " 'theirs',\n",
              " 'that',\n",
              " 'itself',\n",
              " 'and',\n",
              " 'to',\n",
              " 'o',\n",
              " 'hasn',\n",
              " 'below',\n",
              " \"wasn't\",\n",
              " 'myself',\n",
              " \"you're\",\n",
              " 'we',\n",
              " 'other',\n",
              " 're',\n",
              " \"aren't\",\n",
              " 'more',\n",
              " 'didn',\n",
              " 'd',\n",
              " 'after',\n",
              " \"couldn't\",\n",
              " \"you'd\",\n",
              " 'while',\n",
              " 'your',\n",
              " \"it's\",\n",
              " 'both',\n",
              " 'mustn',\n",
              " 'had',\n",
              " 'for',\n",
              " 'doesn',\n",
              " \"shan't\",\n",
              " 'too',\n",
              " 'you',\n",
              " 'are',\n",
              " 'down',\n",
              " 'all',\n",
              " 'won',\n",
              " 'hers',\n",
              " 'should',\n",
              " \"shouldn't\",\n",
              " 'again',\n",
              " 'll',\n",
              " 'above',\n",
              " 'i',\n",
              " 'because',\n",
              " 'out',\n",
              " \"weren't\",\n",
              " 'during',\n",
              " 'in',\n",
              " 'but',\n",
              " 'those',\n",
              " 'it',\n",
              " 'hadn',\n",
              " \"should've\",\n",
              " 'isn',\n",
              " 'do',\n",
              " 'now',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"isn't\",\n",
              " 'couldn',\n",
              " 'be',\n",
              " 'their',\n",
              " 'once',\n",
              " 'will',\n",
              " 'y',\n",
              " 'than',\n",
              " \"that'll\",\n",
              " 'nor',\n",
              " \"mightn't\",\n",
              " 'his',\n",
              " 'this',\n",
              " 'if',\n",
              " 'them',\n",
              " 'been',\n",
              " 'these',\n",
              " 'themselves',\n",
              " \"didn't\",\n",
              " 'of',\n",
              " \"haven't\",\n",
              " \"you'll\",\n",
              " 'wouldn',\n",
              " 'an',\n",
              " \"she's\",\n",
              " 'with',\n",
              " 'into',\n",
              " 'aren',\n",
              " 'shan',\n",
              " 'don',\n",
              " 'he',\n",
              " 'they',\n",
              " 'ma',\n",
              " 'any',\n",
              " 'himself',\n",
              " 'the',\n",
              " 't',\n",
              " 'did',\n",
              " 'until',\n",
              " \"hasn't\",\n",
              " \"don't\",\n",
              " 'ours',\n",
              " 'off',\n",
              " 'so',\n",
              " 'between',\n",
              " 'him',\n",
              " \"hadn't\",\n",
              " 'on',\n",
              " 'being',\n",
              " \"won't\",\n",
              " 'am',\n",
              " \"doesn't\",\n",
              " 'own',\n",
              " 'each',\n",
              " 'its',\n",
              " 'before']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\""
      ],
      "metadata": {
        "id": "-bWYNzqAJ09Q"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_after_stopword_removal = [token for token in sentence.split() if token not in sw]\n",
        "\" \".join(sentence_after_stopword_removal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xSE1Gc58KSi3",
        "outputId": "eea2aefd-67fd-4975-a4e7-71ba6452cc39"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'putting efforts enhance understanding Lemmatization'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "s = \"Natural Language Processing is the way to open many doors of Industry\"\n",
        "tokens = s.split()\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "[\" \".join(token) for token in bigrams]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZG20hf-Kbst",
        "outputId": "611b6f90-4c4b-446e-b7f6-533713b45e8c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language',\n",
              " 'Language Processing',\n",
              " 'Processing is',\n",
              " 'is the',\n",
              " 'the way',\n",
              " 'way to',\n",
              " 'to open',\n",
              " 'open many',\n",
              " 'many doors',\n",
              " 'doors of',\n",
              " 'of Industry']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "s = \"Natural Language Processing is the way to open many doors of Industry\"\n",
        "tokens = s.split()\n",
        "bigrams = list(ngrams(tokens, 3))\n",
        "[\" \".join(token) for token in bigrams]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSGZn6dbMRI5",
        "outputId": "af62535b-f372-40c7-dbed-a7548e61ef75"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language Processing',\n",
              " 'Language Processing is',\n",
              " 'Processing is the',\n",
              " 'is the way',\n",
              " 'the way to',\n",
              " 'way to open',\n",
              " 'to open many',\n",
              " 'open many doors',\n",
              " 'many doors of',\n",
              " 'doors of Industry']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "s = \"Natural Language Processing is the way to open many doors of Industry\"\n",
        "tokens = s.split()\n",
        "bigrams = list(ngrams(tokens, 4))\n",
        "[\" \".join(token) for token in bigrams]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuXh1EaWMXul",
        "outputId": "08406eef-574e-4f9e-9c79-27daaea85574"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language Processing is',\n",
              " 'Language Processing is the',\n",
              " 'Processing is the way',\n",
              " 'is the way to',\n",
              " 'the way to open',\n",
              " 'way to open many',\n",
              " 'to open many doors',\n",
              " 'open many doors of',\n",
              " 'many doors of Industry']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttgTJQI-PTn9",
        "outputId": "aadda894-486f-4a96-d773-793f9f6b0aac"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/zomato_reviews.csv\")\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ma1Q0xrGSpLn",
        "outputId": "ae1e4737-5ec3-49f7-d33a-6cf01bb7db53"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Review sentiment\n",
              "0  Virat Kohli did a great thing to open his rest...  positive\n",
              "1  This place have some really heathy options to ...  positive\n",
              "2  Aerocity is the most finest place in Delhi for...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35edec3f-ec3c-4200-a426-e1736c448805\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Virat Kohli did a great thing to open his rest...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This place have some really heathy options to ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aerocity is the most finest place in Delhi for...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35edec3f-ec3c-4200-a426-e1736c448805')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-35edec3f-ec3c-4200-a426-e1736c448805 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-35edec3f-ec3c-4200-a426-e1736c448805');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus  = pd.Series(df.Review.tolist()).astype(str)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZZFylUNS0Hi",
        "outputId": "3c2c8219-9b76-44b5-bfab-b0c135b81c10"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Virat Kohli did a great thing to open his rest...\n",
              "1       This place have some really heathy options to ...\n",
              "2       Aerocity is the most finest place in Delhi for...\n",
              "3       Yesterday evening there was small team lunch ,...\n",
              "4       I find aerocity to be the best place in delhi ...\n",
              "                              ...                        \n",
              "1591    || DESI LANE || So we were at alipore's most h...\n",
              "1592    \"Desi Lane\" is one of the most trending place ...\n",
              "1593    One of the cool and pocket pinch restaurant at...\n",
              "1594    \"DESI LANE\" one of the best places in town and...\n",
              "1595    Looking for good place for lunch but dont wann...\n",
              "Length: 1596, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Cleaning (Removal of special characters/punctuations & case folding)"
      ],
      "metadata": {
        "id": "Ufr_EMeaUCj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "    \n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "    \n",
        "    Output : Returns the cleaned text corpus\n",
        "    \n",
        "    '''"
      ],
      "metadata": {
        "id": "N7xJgZSCUjcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "  cleaned_corpus = pd.Series()\n",
        "  for row in corpus:\n",
        "    qs = [] #for adding all splitted words\n",
        "    for word in row.split():\n",
        "      if word not in keep_list:\n",
        "        p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "        p1 = p1.lower()\n",
        "        qs.append(p1)\n",
        "      else:qs.append(word)  \n",
        "      cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))#adding series in cleaned_corpus by joining all splitted words\n",
        "    return cleaned_corpus  "
      ],
      "metadata": {
        "id": "9ATcD9n8T5wt"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stopwords Removal"
      ],
      "metadata": {
        "id": "VaLKluBLWSBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_removal(corpus):\n",
        "  wh_words = ['who','what','when','why','how','which','where','whom']\n",
        "  stop  = set(stopwords.words('english'))  ##removing wh_words from stopwords\n",
        "  for word in wh_words:\n",
        "    stop.remove(word)\n",
        "  corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "  return corpus  "
      ],
      "metadata": {
        "id": "y9AzwrVrWRAj"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lemmatization"
      ],
      "metadata": {
        "id": "_BkmuSY-X2Eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(corpus):\n",
        "  wnl = WordNetLemmatizer()\n",
        "  corpus = [[wnl.lemmatize(x) for x in x]for x in corpus]\n",
        "  return corpus"
      ],
      "metadata": {
        "id": "LyIy-kK3Xw3y"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming"
      ],
      "metadata": {
        "id": "TlkfN23BYvtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PorterStemmer - used for english text , used to remove suffixes\n",
        "\n",
        "##SnowballStemmer - used for non-englisgh text, used to removes suffixes as well as find the root word"
      ],
      "metadata": {
        "id": "TG_wkiZIZxwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(corpus, stem_type=None):\n",
        "  if stem_type == 'snowball':\n",
        "    stemmer = SnowballStemmer(language='english')\n",
        "    corpus = [[stemmer.stem(x) for x in x]for x in corpus]\n",
        "  else:\n",
        "    stemmer = SnowballStemmer(language='english')\n",
        "    corpus = [[stemmer.stem(x) for x in x]for x in corpus]\n",
        "  return corpus    "
      ],
      "metadata": {
        "id": "Fch5E2stYrZa"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "    \n",
        "    Input : \n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "    \n",
        "    Note : Either stemming or lemmaPurpose : Function to perform all pre-processing tization should be used. There's no benefit of using both of them together\n",
        "    \n",
        "    Output : Returns the processed text corpus"
      ],
      "metadata": {
        "id": "mboE5Pb8a0Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus,keep_list,cleaning = True, stemming = False,stem_type = None,lemmatization = False, remove_stopwords = True):\n",
        "  if cleaning == True:\n",
        "    corpus = text_clean(corpus,keep_list)\n",
        "\n",
        "  if remove_stopwords == True:\n",
        "    corpus = stopwords_removal(corpus) \n",
        "  else:\n",
        "     corpus = [[x for x in x.split()]for x in corpus]\n",
        "\n",
        "  if lemmatization == True:\n",
        "     corpus = lemmatize(corpus)\n",
        "\n",
        "  if stemming == True:\n",
        "     corpus = stem(corpus, stem_type) \n",
        "\n",
        "     corpus = [' '.join(x) for x in corpus]   \n",
        "\n",
        "     return corpus\n"
      ],
      "metadata": {
        "id": "CbOTZtr8YrGg"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_dot_words = ['U.S.A','Mr.','Mrs.','D.C.']"
      ],
      "metadata": {
        "id": "TteCuT_HcmGQ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Preprocessing with Lemmatization here\n",
        "corpus_with_lemmatization = preprocess(corpus,keep_list = common_dot_words,stemming = False,stem_type = None,lemmatization =True,remove_stopwords=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VqRHoirdAg3",
        "outputId": "b010397f-d809-4751-9ec7-ed6aa32e4e5a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-b99b7f51cf9d>:2: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Preprocessing with Stemming here\n",
        "corpus_with_stemming = preprocess(corpus,keep_list = common_dot_words,stemming=True,stem_type = 'snowball',lemmatization = False,remove_stopwords = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlNPbVhednfT",
        "outputId": "f3990d7e-4e58-48db-c124-4c61e71d3ea6"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-b99b7f51cf9d>:2: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the results on applying\n",
        "1. Lemmatization\n",
        "2. Stemming\n",
        "Note: Stopwords removal and text cleaning have been applied on both the occassions."
      ],
      "metadata": {
        "id": "9y3omYvY6CT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original string: \",corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UXaBW5JENIf",
        "outputId": "af28da0f-6d20-444f-9028-e8ced3c3de6b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original string:  Virat Kohli did a great thing to open his restaurant in an exquisite place of Delhi. Wide range of food with lots and lots of options on drinks. Courteous staff with a quick response on anything.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"String after stemming: \", corpus_with_stemming[0])"
      ],
      "metadata": {
        "id": "JLe-g55FGlHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27232e5-8713-4311-bebe-9179d2856f18"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String after stemming:  virat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"String after lemmatiization: \", corpus_with_lemmatization[0])"
      ],
      "metadata": {
        "id": "BUxYooSNYzXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7AaBfO8AY64O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}